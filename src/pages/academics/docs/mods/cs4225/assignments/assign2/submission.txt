====== Part 1 ======

(brief 1-2 line explanation of your solution here)
1. alpha begins with "alpha>", beta begins with "[type]", so gamma are remaining messages
2. we filter out alpha and beta rows using .filter(df.value.contains("alpha>") == False),
   replacing "alpha>" with "[notice]" and "[error]" for beta
3. we filter for gamma rows containg "Invalid user" with .filter(df.value.contains("Invalid user"))
4. there is only 1 row output, which is [Row(value='Invalid user Engteampw! from 103.79.141.86')]

import findspark
findspark.init()
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[*]").getOrCreate()

df = spark.read.text("system.log").repartition(1).cache()
df.toPandas()

df.filter(df.value.contains("alpha>") == False)\
  .filter(df.value.contains("[error]") == False)\
  .filter(df.value.contains("[notice]") == False)\
  .filter(df.value.contains("Invalid user")).collect()

====== Part 2 ======

1. Drop duplicate [src, dst] to reduce our search space
2. We know servers that are accessed on non multiples of 7 days must not be the server
3. Get all the servers we know are wrong with df.filter(df.day % 7 != 0)
4. Exclude those servers with .join(wrong, ["dst"], "leftanti")
5. Finally the server must be accessed by all 13 members, so we group them with .groupBy("dst")
6. If all 13 members accessed the server, the count will be 13, we count with .count()
7. Filter servers with less than 13 unique src's with .filter(possible["count"] == 13)
8. Only 1 row is left, [Row(dst='207.025.071.025', count=13)]

import findspark
findspark.init()
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[*]").getOrCreate()

df = spark.read.option('header', True).csv("network.csv").repartition(1).cache()
df.toPandas()

wrong = df.filter(df.day % 7 != 0)
possible = df.dropDuplicates(["src", "dst"]).join(wrong, ["dst"], "leftanti").groupBy("dst").count()
possible.filter(possible["count"] == 13).collect()